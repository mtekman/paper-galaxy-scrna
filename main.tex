%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Please note that whilst this template provides a
% preview of the typeset manuscript for submission, it
% will not necessarily be the final publication layout.
%
% letterpaper/a4paper: US/UK paper size toggle
% num-refs/alpha-refs: numeric/author-year citation and bibliography toggle

%\documentclass[letterpaper]{oup-contemporary}
\documentclass[a4paper,num-refs]{oup-contemporary}

%%% Journal toggle; only specific options recognised.
%%% (Only "gigascience" and "general" are implemented now. Support for other journals is planned.)
\journal{gigascience}

\usepackage{graphicx}
\usepackage{siunitx}

%%% Flushend: You can add this package to automatically balance the final page, but if things go awry (e.g. section contents appearing out-of-order or entire blocks or paragraphs are coloured), remove it!
% \usepackage{flushend}

\title{A single-cell RNA-seq analysis suite using the Galaxy Framework}

%%% Use the \authfn to add symbols for additional footnotes, if any. 1 is reserved for correspondence emails; then continuing with 2 etc for contributions.
\author[1,\authfn{1},\authfn{2}]{Mehmet Tekman}
\author[2,\authfn{1},\authfn{2}]{Second Author}
\author[2]{Third Author}
\author[1,\authfn{1}]{Rolf Backofen}

\affil[1]{Chair of Bioinformatics, University of Freiburg, Freiburg, Germany, }
\affil[2]{Second Institution}

%%% Author Notes
\authnote{\authfn{1}tekman@informatik.uni-freiburg.de;backofen@informatik.uni-freiburg.de}
\authnote{\authfn{2}Contributed equally.}

%%% Paper category
\papercat{Paper}

%%% "Short" author for running page header
\runningauthor{Tekman et al.}

%%% Should only be set by an editor
\jvolume{00}
\jnumber{0}
\jyear{2017}

\begin{document}

\begin{frontmatter}
\maketitle
\begin{abstract}
%% No more than 250 words! (current 414)
\textbf{Background} The turbulent ecosystem of single-cell RNA-seq tools has up to recently been plagued by a plethora of diverging analysis strategies, inconsistent file formats, and a wide range of compatibility issues between different software packages. Due to the initial sparseness of the data caused by low coverage and intrinsic cell noise meant that strategies could be developed on very few compute resources to solve the problems of normalisation, confounder removal, and clustering. The differing sequencing protocols produced by the various small labs has produced many different types of datasets, and specific strategies to analyse them. This has led to a saturation of different scRNA tools, many of which see very little use. The rise of 10X Genomics datasets that are now saturating the field has provided a new standard that has refocused the energies of the different analysis pipelines towards analysis consistency, and has once again driven the demand for large computing requirements to cluster the increasingly growing datasets.

Many of the tools have different libraries

Given the cluster-based interpretive nature of an scRNA-seq analysis, it is not always clear to initial researchers whether the steps they have undertaken to get to the final clustering are necessarily correct. The disparity between the statistically-driven algorithmic nature of the analysis to the underlying cell biology is a frightening prospect for many researchers first introduced into the topic.

\textbf{Results}
The Galaxy bioinformatic reproducible computing framework addresses both of these paradigms, by providing tools, workflows and trainings that enables users to perform one-click 10X pre-processing, and also empowers them to de-multiplex raw sequencing data for more custom setups. The resulting downstream analysis is supported by a range of high quality interoperable suites separated into common stages of analysis: inspection, filtering, normalization, confounder removal and clustering. The teaching resources cover a host of different concepts with the core focus of unifying the algorithms and statistics with the underlying cell biology. Access to all resources is provided at the https://singlecell.usegalaxy.eu portal.

\textbf{Conclusions}
The reproducible and training-oriented ethos of the Galaxy community and framework provides a sustainable HPC environment for users to run flexible analyses on both 10X and smaller datasets. The scRNA-oriented trainings within the Galaxy Training Network paired with the frequent training workshops hosted by the Galaxy Team provides a means to for users to be taught and to teach others to navigate through the complexities of a scRNA-seq analysis.
\end{abstract}

\begin{keywords}
%% 3- 10 keywords
scRNA; Galaxy; resources; HPC; single-cell; 10X
\end{keywords}
\end{frontmatter}

%%% Key points will be printed at top of second page
\begin{keypoints*}
\begin{itemize}
\item The analysis of scRNA is currently stabilising towards 10X Genomics datasets
\item A rapid
\item One last point.
\end{itemize}
\end{keypoints*}

* accessiblity, scalability
* Fileformats, compatibility and conversions, interconvertible
* Visualisations on top
* Plotting tools on top of LOOM
* New workflows based upon the existing, we encourage it

* Talk about the non-standaridsation of the pre-processing -- cannot fix it, and we encourage to stick to certain standards, and our tools ar eflexbile in that regard, pipe itinto a scalable pipeline. Cherr
Paragraph at the end.
Collaborative effort, worldwide -- nicola, maria

Single-cell RNA-seq has provided a new means to peer into the underlying functional mechanisms that is soon to redefine the field of transcriptomics. The insights and new cellular resolution of the analyses enable researchers to follow the differentiation paths a cell may take as it develops from a stem cell to a mature cell type, as well as exploring the many factors that can influence the decisions that a cell makes to decide its fate.

Over a hundred different packages and pipelines have been developed to assist researchers in the analysis of scRNA-seq, but many of these are computationally and statistically complex for the average biololgically-minded researcher, and thus the uptake of many of these packages is dependent on whether the methods come from the main downstream analysis pipelines such as Seurat or ScanPy, or whether the methods from these packages have been incorporated into them.

The reason for this is three-fold: with differing formats and inter-package incompatibility being a major factor, since no one will work with a package that creates its own container format or is not compatible with some of the main formats such as Loom, AnnData, or SingleCellExperiment; platform incompatibilites, where much of the software was written in R and therefore pipelines were driven via R/Bash pipelines, whereas actual pipeline frameworks such as snakemake, Galaxy, or CWL are more python-driven and use more transparent formats; Different pipelines produce different results, where the stochastic nature of the analyses means that any uncertainty in a crucial quality control stage upstream (such as filtering) will propagate forward into the downstream sections to yield different results. This uncertainty, and the statistically-driven methods to overcome them leaves a wide information gap for researchers simply trying to understand the underlying dynamics of a cell.

Thankfully there are many online resources, jupyter notebooks that can walk a researcher through how to perform an analysis as well as understanding the concepts at each stage, but not many make use of reproducible environments either via Conda or other.

Galaxy is an open source biocomputing infrastructure that encapsulates the three main tenets of science; reproducibility, peer review, and open-access, all freely accessible within a web-browser. It hosts a wide range of highly-cited bioinformatic tools with many different versions, enables users to freely create their own workflows via a seamless drag-and-drop interface, and provides teaching materials on a whole slew of various different topics and sub-topics. Trainings and workshops are hosted at least twice a year that encourage users to play with and understand their data.

The teaching and training materials are part of the Galaxy Training Network (GTN) which is a worldwide collaborative effort to produce high-quality teaching material in order to educate users in how to analyse their data, and in turn to train others of the same materials via easily deployable workshops. The Galaxy project provides a priority processing queue for anybody who may require it during a workshop. The GTN has grown rapidly since it's conception and gains new volunteers every year who each contribute and coordinate training and teaching events, maintain topic and subtopics, and provide peer review on new material.

The training materials cover several topics and are written in a flexible markdown format that empowers contributors to write tutorials on their expert topics without resorting to bloated editors, whilst enabling the community to peer-review such contributions easily via git or on Github by using standard diff utilities. Tutorials usually consist of a hands-on workflow that guides the user through an analysis with Galaxy utilising a step-by-step approach, and is often accompanied with a slide deck that either serve to explain standalone concepts more concisely, or are used during workshops and trainings as a way to introduce the user to the topic. In an effort to maintain reproducibility in science,  all tutorials require example workflows, and all materials required to run the workflows and tutorials are hosted for free open access at Zenodo with a DOI tag.

The Galaxy tools and the GTN are further tied together by Galaxy subdomains, that better encapsulate the various topics into their own self-contained spheres of influence. These complement the training material by providing only the necessary Galaxy tools from the total set of Galaxy utilities such that the toolbar is not over-cluttered with tools that are not so relevant to the material (e.g. Variant Analysis tools being included in scRNA-seq resources), and also the benefit that smaller specialised Galaxy instances can be deployed with much smaller redundancy since only the core tools (e.g. text manipulation, FASTQ quality control, etc.) and the topic-specific utilities are packaged with it.

In this light, the \url{singlecellomics.usegalaxy.eu} subdomain hosts the entirety of the single-cell materials, tools, workflows, and single-cell related events, and it is this which is the focus of the paper. Given the extremely broadening scope of scRNA-seq, the Human Cell Atlas also have their own subdomain in this regard and maintain their materials at \url{humancellatlas.usegalaxy.eu}.

The tutorials are split into two main parts that describe the post-processing and downstream analysis, referring to the distinct stages of constructing a count matrix from initial sequencing data, and then performing cluster analysis on the count matrix. These stages are very far from one another in terms of their information content, since the pre-processing stage requires the researcher to have more wetlab knowledge than a bioinformatician would normally need, and the downstream analysis stage requires the researcher to be familiar with machine learning concepts that a wetlab scientist might not be too familiar with.

The tutorial are designed to appeal to both types of researchers, the biologist and the statistician such that each can benefit from the other's knowledge and, most importantly, bridge the knowledge gaps that separate the two.

The pre-processing scRNA-seq materials tackle the two most common use-cases that researchers will encounter when they first begin the field: processing scRNA-seq data from 10x Genomics, and processing data generated from other protocols.

Before the era of 10x Genomics, scRNA-seq data had to be demultiplexed, mapped, and quantified. The demultiplexing stage requires an intimate knowledge of cell barcodes and Unique Molecular Identifiers (UMIs) which are protocol dependent and requires that the bioinformatician knows exactly where and how the data they are processing were generated. One common pitfall at the very first stage is determining exactly how many cells to expect in the FASTQ input data, and this requires threee crucial piece of information: which read contains the barcodes (or which subset of both the forward and reverse reads contains the barcodes); which specific barcodes were used in the analysis, and determining the number of acceptable barcode mismatches/errors and how to resolve or cluster them.

Naive strategies involve using a known barcode template and querying against the FASTQ to profile the number of reads that align to a specific barcode, often employing 'knee' methods to estimate this amount. However, this approach ultimately fails since certain cells are more likely to be over-represented compared to others, and some cell barcodes may contain more un-mappable reads compared to others meaning that the metric of higher read library sizes are not neccesarily correlated with a better-defined cell. Ultimately, the bioinformatician must go to the sequencing lab and ask them which cell barcodes were used, as these are often not specific to the protocol but to the technician who designed them, with the idea that they should not align to the reference genome or transcriptome.

10x Genomics appeared in the field in XXXXDATE, and steadily gained traction amongst researchers due to the high-throughput and high-quality datasets they generated. The initial state of the scRNA-seq software ecosystem was in a state of disarray before the emergence of 10x, due to the different competing, often not inter-exchangeable, R-based formats which required researchers to dig deeper into the internal semantics of R, instead of the internal biological variability of their datasets. 10x Genomics streamlined much of this process by popularising their own transparent H5-based format as well as their own \prog{Cell Ranger} pipeline.

The \prog{RNA STARsolo} tool is an update to the \prog{RNA STAR} utility designed to be a drop-in replacement for the \prog{Cell Ranger} pipeline, that boasts a ten-fold speedup of their pipeline. Though the memory requirements are also an order of magnitude higher, they are easily catered for using the Galaxy Framework which does not suffer from such memory constraints, and has the benefit of not requiring the Illumina lane read information to perform the processing either. The training and workflow for this is therefore quite straightforward and follows the same mode of discovery and analysis.
%TODO coordinate with Alex
In a similar vein, there is also an Alevin-based workflow which also performs the demultiplexing, alignment (without mapping), and quantification in a single step to produce a count matrix. This is ideally paired with the downstream Monocle workflow to perform the full analysis, but the choice is left to the user due to the interexchangeability of the file formats being generated.

The custom pre-processing uses the CELSeq2 protocol following the barcoding strategies at the local Max-Planck Freiburg laboratory as its main template, but the training is flexible to accommodate any droplet or well-based protocol. The training pictographically walks users through the concepts of extracting cell barcodes, both at the over-archingly conceptial and basic file previewing level, and elucidates the use of UMIs and its role and significance in the process of read deduplication, and explains and instructs the user in the process of performing further quality controls on their data during the post-mapping process via \prog{RNA STAR} and basic BAM Filter utilities that are native to Galaxy. At each stage, the user's knowledge is queried via expandable Question Box dialogs, and helpful hints for future processing are given via Comment Boxes, all written in transparent Markdown to the specification of the GTN contributing guidelines designed to aid users.

The downstream modules are defined by the five main stages of downstream scRNA-seq analysis as defined by the Hemberg group; filtering, normalisation, confounder removal, clustering, and trajectory inference. There are three workflows to aid in this process, each sporting a different well-established scRNA-seq pipeline tool. The Scater pipeline was contributed by the XXXX group, and follows an intuitive visualise-edit-visualise paradigm which provides a very understandable means to perform further quality control on a count matrix by use of repeated incremental changes on a dataset through the use of PCA and library size based metrics. Once this pre-analysis stage is complete and that the user is now confident with the quality of their data, they can perform the analysis via one or both of two comprehensive workflows: RaceID, and ScanPy.

The RaceID package was developed initially to analyse rare cell transcriptomes whilst being robust against noise, and thus is ideal for working with smaller datasets in the range of 300 to 1000 cells, though due to its impressive cell lineage and fate predictions models it can also be used on larger datasets albeit with some scaling costs. The ScanPy pipeline was developed as the Python counter response to overinflated R package ecosystem for scRNA-seq which was the dominant language for such analyses, and it was one of the first packages with native 10x genomics file format support. Since then it has grown substantially, and has been re-implementing much of the newer methods released in BioConductor, therefore providing a single source to perform many different types of the same analysis and becoming a dominant leader in the field where others have been slow to adapt.

Both workflows guide the user from the initial filtering step (if required) through to the clustering and final trajectory inference and inspection, mostly through a guided linear path because historically different pipelines do not work so well with one another.
% TODO!!
However, through the use of a convenience wrapper, the different modules of each pipeline are now able to communicate with one another via the AnnData datatype. The native RaceID R datatypes are converted into AnnData so that the analysis via one workflow can be continued within another, should the user wish to reap the benefits of one or more modules from another pipeline.

The modules emulate the five main stages of analysis mentioned previously, where filtering, normalisation, and confounder removal are typically separated into distinct stages. During the filtering stage, the initial count matrix removes low-quality or unwanted cells using commonly used parameters such as minimum gene detection sensitivity and minimum library size, and low-quality genes are also removed under similar metrics, where the minimum number of cells for a gene to be included is decided. The Scater workflow also offers a PCA-based method to help with the feature selection so that only the highly variable genes are left in the analysis.

There is always the danger of over-filtering a dataset, and indeed some normalisation methods rely on a background noise model generated from the expression of less variable or housekeeping genes, so it is important that the user first performs a naive analysis and only refines their analysis to boost the signal-to-noise ratio.

The normalisation of a step aims to remove any technical factors that are not relevant to the analysis, such as the library size, where cells of the same time are likely to differ from one another more by the number of transcripts they exhibit due to several factors that can affect it. The first and foremost is cell capture efficiency, where different cells produce more or less transcripts based on the amplification and coverage conditions they are sequenced in. The second is the presence of dropout events which manifest as counts of ``zeroes'' in the final count matrix but are uncertainly derived from the molecule existing in the cell and simply not being detected, or that the molecule itself was not present. This uncertainty alone led to a slew of different normalisation techniques that try to model this expression either via hurdle models, or imputing the data via manifold learning techniques, or working around the issue by pooling subsets of cells together.

In this regard, both the RaceID and ScanPy workflows offer many different options for normalisation and users are encouraged to take advantage of the branching workflow model of Galaxy to explore all possible avenues.

Other sources of variability stem from unwanted biological contributions known as confounder effects, such as cell cycle effects and transcription. Depending on what stage of the cell cycle a cell was sequenced at, two cells of the same type might cluster differently simply because one might have more transcripts due to being in the M-phase of the cell cycle. Library sizes not withstanding, it is the variability in specific cell cycle genes that can be the main driving factor in the overall variability. Thankfully, these effects are linear and are therefore quite easy to regress out, and we replicate an entire standalone ScanPy workflow dedicated to detecting and visualising the effects based on the original notebook.

The transcription effects are harder to model, as these are semi-stochastic and are as of yet still not well understood. In bulk RNA-seq the expression of genes undergoing transcription are averaged to give ``high'' or ``low'' signals producing a global effect that gives the false impression that transcription is a continuous process. The reality is more complex; where cells undergo transcription in ``bursts'' of activity followed by periods of no activity, at irregular intervals. At the bulk level these discrete processes are smoothed to give a continuous effect, but at the cell level it could mean that even two directly adjacent cells of the same type normalised to the same number of transcripts can still have wildly different levels of expression for a gene due to this process. This is not something that can be countered for, but it does educate the users in the factors that they can and cannot control in an analysis, and how much variability they can expect to see.

Once a user has obtained a count matrix they are confident in, they can perform clustering where they are first educated in the use of commonly used clustering techniques such as k-means and hierarchical clustering, as well as dimension reduction techniques, through the use of helpful images and community examples.
%TODO
In the ScanPy tutorial, the louvain clustering approach is explained via a standalone slide deck to assist in the workflow.

The clustering and the cluster inspection are notably separated into distinct utilities here, with the understanding that the same initial clustering can appear differently under different projections, for example tSNE or UMAP. Ultimately the user is encouraged to play with the plotting parameters to yield the best looking clusters for the same static clustering parameters. For tSNE this often means adjusting the perplexity parameter without having to rerun the entire tSNE clustering computation, though UMAP also has such parameters.

The inspection wrappers reflect the modules offered by the native packages, with package-specific information overlayed on top of the map projections in the manner that the package authors assumed was best. However, the AnnData and LOOM specification store this map projection data separately, and so the user is not at the mercy of these plotting tools, with plenty of HTML5-based interactive visualisations available at their disposal such that the user can query individual cell features without having to generate static images via tools such as \prog{cellxgene} which are also available on the Galaxy server. Though these tools are excellent at dynamically displaying map projections, especially 3-dimensional ones, further computation must be performed to perform a full pseudotime analysis. 

The cell pseudotime series analysis is often referred to as the trajectory inference stage, since cells are ordered along a trajectory to reflect the continuous changes to gene expression along a pathway under the assumption that the cells are transitioning from one type to another.

For the trajectory inference stage there is the PAGA graph abstraction technique championed by ScanPy, and there is also the FateID and StemID packages for the RaceID workflow. The former provides a level of graph abstraction to the datasets in order to infer a community graph structure which it can use to learn the shape of the data and infer pathways between neighbourhoods. The latter is more intuitive, in that it constructs a minimal spanning tree of related clusters that infer lineage, and cell fate decisions can be explored by individually querying branches in this tree, as a function of the genes which are up or down regulated along the currently explored pathway. The statistical strength and significance of each pathway guides the user along more valid trajectories that would more accurately reflect the biological variation occurring within transitioning cells.

The insights and novel cell types discovered in these analyses can also be integrated into the Human Cell Atlas portal, which is an initiative that aims to classify unique or rare cell types and their transitive properties in order to build a comprehensive map of cells that can be used to inverstigate the various differentiation pathways of multipotent stem cells.

The single-cell materials on the GTN are growing substantially every year, with at first only one pre-processing tutorial in 2018, one downstream tutorial at the start of 2019, and at the current time of writing three pre-processing tutorials and three downstream analysis workflows, further accompanied by visualisations. The first single-cell workshop was given internally within the Freiburg MeInBio consortium, but is now available at the [Feb. meeting thing] and the summer and winter Freiburg Galaxy workshops.

With single-cell RNA firmly within the Galaxy framework, echoes the efforts to standardise the field in order to promote reproducible research. The size of the datasets are substantial, and the computing resources required to handle them are growing as the sequencing technology scales, requiring more and more researchers to migrate towards cloud-based solutions in order to reap the benefits of superior hardware; computing abilities, storage requirements, and most of all data redundancy.

Those looking for stability in the field would have to look hard to find it, due to the non-standardisation of the R package ecosystem in which most of the analysis packages were written. Galaxy abstracts the user from the format specifics, by exposing the user only to the tool and data mindset, where tools simply ingest and produce data.

The community comes together regularly during scheduled CoFests to review, contribute, and actively maintain the training materials, and the number of volunteers is growing every year with named roles assigned to interested parties. This upkeep ensures that the Galaxy resources will continue to outlive other materials which typically decay over time once interest in them declines. By visiting and revisiting material in these scheduled slots, the material can only continue to improve, and now reaches even more audiences via the use of language translation tools which ensures that the international community is never left out.

Overall, the materials and tools presented in this paper represent the first fully comprehensive interactive guide to producing a single-cell RNA-seq analysis from the data capture stage all the way to the final publication, with the GTN community fully supporting the user the entire way. The materials grow with the user's knowledge, and expert users are known to give back to the material, enabling a constructive cycle of knowledge that emulates the ideals of open and transparent science through the use of web-based tools and a strong biocomputing framework to support it.

Each of the trainings seamlessly leads on from topic to the next, with the markdown encouraging authors to set tutorial pre-requisities and follow up tutorials, all encapsulated within the single topic. Not only does this allow users to derive a seamless route through the flurry of tutorials, but it also enables a branching tree structure of topics to be generated so that users can guide their own learning. 







Issues:
- Workflow → { preprocessing | preprocessing_tenx } → [scater] → { RaceID | ScanPy } → { Visualisation | Human Cell Atlas }
- Humancellatlas also have a scanpy based workflow, so this sucks...

Talk about:
- Strandedness?


Emphasise more:
- Community, scalability, portability
- 










\section{Introduction to this Template}

\begin{epigraph}{Epigraph source name}
This is the epigraph text, should you like to add one. Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.
\end{epigraph}

This is the \LaTeX{} template for GigaScience journal manuscript submissions. \textbf{Please note that whilst this template provides a preview of the typeset manuscript for submission, it will not necessarily be the final publication layout.}

There are important commands in the preamble that you will need to modify for your own manuscript. If you are using this template on Overleaf, please switch the editor to Source code mode to view them; or if you prefer to stay in the Rich Text view, click on the title in the Rich Text view to display the preamble code.

Use the \verb|\journal{...}| command in the preamble so that the correct journal name, logo and colours are loaded automatically. \textbf{Only certain journals and options are supported at this time;} check with your journal's editorial office if your journal is supported.

Alternatively you can re-define \verb|\jname|, \verb|\jlogo| and the \verb|jcolour| explicitly, though check with your journal's editorial office to confirm that this is appropriate.

Specify your manuscript's category with the \verb|\papercat{...}| command in the preamble.

See the sample code in the preamble for a sample of how author and affiliation information can be specified.

Use later sections starting with `Background' on page~\pageref{sec:background} to write your manuscript. The remainder of this current section will provide some sample \LaTeX{} code for various elements you may want to include in your manuscript.

\subsection{Sectional Headings}
You can use \verb|\section{...}|, \verb|\subsection{...}| commands to add more sections and subsections to your manuscript. Further sectional levels are provided by \verb|\subsubsection|, \verb|\paragraph| and \verb|\subparagraph|.

\subsection{Citations and References}
Use the \verb|num-refs| document class option for numerical citations, and \verb|alph-refs| option for author-year citations.
Use the \verb|\citep| command for parenthetical citations, and \verb|\citet| command for text citations (when using \verb|alpha-refs|).
This is a citation: \citep{Fan:2004} and here are two more: \citep{Cox:1972,Hear:Holm:Step:quan:2006}.

\begin{quote}
This is a quote. Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.
\end{quote}

\begin{itemize}
\item This is a bullet list.
\item Another point.
\item A third point.
\end{itemize}

This\footnote{This is the footnote text. This is the footnote text. This is the footnote text. This is the footnote text. This is the footnote text.} is a footnote. Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.

\begin{itemize}
\item This is a numbered list.
\item Another point.
\item A third point.
\end{itemize}

Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.


\subsubsection{This is a 3rd level heading}

Use \verb|\subsubsection| to get a 3rd level heading.
Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.


\paragraph{This is a 4th level heading}

Use \verb|\paragraph| to get a 4th level heading.
Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Lorem ipsum dolor sit amet, consectetur adipiscing elit.

\subparagraph{This is a 5th level heading}

Use \verb|\subparagraph| to get a 5th level heading.
Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Lorem ipsum dolor sit amet, consectetur adipiscing elit.


\subsection{Figures and Tables}
Figures and tables can be added with the usual \verb|figure| and \verb|table| environments, e.g.~Figure \ref{fig:example} and Table \ref{tab:example}. Use \verb|figure*| and \verb|table*| if you need a two-column wide figure or table, as in Figure \ref{fig:example:wide} and Table \ref{tab:example:wide}.

If you have a very wide table or figure, you can use \texttt{sidewaystable} or \texttt{sidewaysfigure}, as in Table \ref{tab:example:sideways}: this will be rotated sideways and occupy a \emph{single column} on its own.

If your table or figure is both wide and tall (so it wouldn't fit well in a single column with \texttt{sidewaystable} or \texttt{figure}),
you can use \verb|table| or \verb|figure| inside a \verb|landscape| environment for a full-page landscaped alternative. A page break will be inserted \emph{immediately before and after} the \verb|landscape| environment (Table \ref{tab:example:landscape}), so you'll need to carefully position it in a suitable location in your manuscript.

\begin{figure}[bt!] %% preferably at bottom or top of column
\centering
\includegraphics[width=\linewidth]{example-image}
\caption{An example figure}\label{fig:example}
\end{figure}

\begin{table}[bt!]
\caption{An example table.}\label{tab:example}
\begin{tabular}{l r l}
\toprule
Item & Quantity & Notes\\
\midrule
Widgets & 42 & Over-supplied\textsuperscript{*} \\
Gadgets & 13 & Under-supplied \\
\bottomrule
\end{tabular}
\begin{tablenotes}
\item This is a table note.
\item \textsuperscript{*}Another note.
\end{tablenotes}
\end{table}


\subsection{Some Mathematics Sample}

Let $X_1, X_2, \ldots, X_n$ be a sequence of independent and identically distributed random variables with $\text{E}[X_i] = \mu$ and $\text{Var}[X_i] = \sigma^2 < \infty$, and let
%
\begin{equation}
S_n = \frac{X_1 + X_2 + \cdots + X_n}{n}
      = \frac{1}{n}\sum_{i}^{n} X_i
\end{equation}
%
denote their mean. Then as $n$ approaches infinity, the random variables $\sqrt{n}(S_n - \mu)$ converge in distribution to a normal $\mathcal{N}(0, \sigma^2)$.


\section{Background}
\label{sec:background}

The background section should be written in a way that is accessible to researchers without specialist knowledge in that area and must clearly state---and, if helpful, illustrate---the background to the research and its aims. The section should end with a brief statement of what is being reported in the article.


\section{Data Description}

A statement providing background and purpose for collection of these data should be presented for readers without specialist knowledge in that area. A brief description of the protocol for data collection, data curation and quality control, as well as potential uses should be included, as well as outlining how the data can be accessed if it is not deposited in our repository.

\begin{figure*}%[b!]  %% Add a [b!] if you prefer the wide image to be at the bottm of the page
\centering
\includegraphics[width=.7\textwidth]{example-image}
\caption{An example wide figure. Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.
}\label{fig:example:wide}
\end{figure*}


\section{Analyses}

This section should provide details of all of the experiments and analyses that are required to support the conclusions of the paper. The authors should make clear the goal of each analysis and state the basic findings.


\begin{table*}[bt!]
\caption{Automobile land speed records (GR 5-10)}\label{tab:example:wide}
% Use "S" column identifier (from siunitx) to align on decimal point.
% Use "L", "R" or "C" column identifier for auto-wrapping columns with tabularx.
\begin{tabularx}{\linewidth}{S l l l r L}
\toprule
{Speed (mph)} & {Driver} & {Car} & {Engine} & {Date} & {Extra comments}\\
\midrule
407.447     & Craig Breedlove & Spirit of America          & GE J47    & 8/5/63  & (Just to demo a full-width table with auto-wrapping long lines) \\
413.199     & Tom Green       & Wingfoot Express           & WE J46    & 10/2/64  \\
434.22      & Art Arfons      & Green Monster              & GE J79    & 10/5/64  \\
468.719     & Craig Breedlove & Spirit of America          & GE J79    & 10/13/64 \\
526.277     & Craig Breedlove & Spirit of America          & GE J79    & 10/15/65 \\
536.712     & Art Arfons      & Green Monster              & GE J79    & 10/27/65 \\
555.127     & Craig Breedlove & Spirit of America, Sonic 1 & GE J79    & 11/2/65  \\
576.553     & Art Arfons      & Green Monster              & GE J79    & 11/7/65  \\
600.601     & Craig Breedlove & Spirit of America, Sonic 1 & GE J79    & 11/15/65 \\
622.407     & Gary Gabelich   & Blue Flame                 & Rocket    & 10/23/70 \\
633.468     & Richard Noble   & Thrust 2                   & RR RG 146 & 10/4/83  \\
763.035     & Andy Green      & Thrust SSC                 & RR Spey   & 10/15/97\\
\bottomrule
\end{tabularx}

\begin{tablenotes}
\item Source is from this website: \url{https://www.sedl.org/afterschool/toolkits/science/pdf/ast_sci_data_tables_sample.pdf}
\end{tablenotes}
\end{table*}


\section{Discussion}

The discussion should spell out the major conclusions and interpretations of the work, including some explanation on the importance and relevance of the dataset and analysis. It should not be restatement of the analyses done and their basic conclusions. The discussion section can end with a concluding paragraph that clearly states the main conclusions of the research along with directions for future work. Summary illustrations can be included.

\begin{sidewaystable}
\caption{Automobile land speed records (GR 5-10). This is the same table as before, but rotated sideways.}
\label{tab:example:sideways}
% Use "S" column identifier (from siunitx) to align on decimal point.
% Use "L", "R" or "C" column identifier for auto-wrapping columns with tabularx.
\begin{tabularx}{\linewidth}{S l l l r L}
\toprule
{Speed (mph)} & {Driver} & {Car} & {Engine} & {Date} & {Extra comments}\\
\midrule
407.447     & Craig Breedlove & Spirit of America          & GE J47    & 8/5/63  & (Just to demo a full-width table with auto-wrapping long lines) \\
413.199     & Tom Green       & Wingfoot Express           & WE J46    & 10/2/64  \\
434.22      & Art Arfons      & Green Monster              & GE J79    & 10/5/64  \\
468.719     & Craig Breedlove & Spirit of America          & GE J79    & 10/13/64 \\
526.277     & Craig Breedlove & Spirit of America          & GE J79    & 10/15/65 \\
536.712     & Art Arfons      & Green Monster              & GE J79    & 10/27/65 \\
555.127     & Craig Breedlove & Spirit of America, Sonic 1 & GE J79    & 11/2/65  \\
576.553     & Art Arfons      & Green Monster              & GE J79    & 11/7/65  \\
600.601     & Craig Breedlove & Spirit of America, Sonic 1 & GE J79    & 11/15/65 \\
622.407     & Gary Gabelich   & Blue Flame                 & Rocket    & 10/23/70 \\
633.468     & Richard Noble   & Thrust 2                   & RR RG 146 & 10/4/83  \\
763.035     & Andy Green      & Thrust SSC                 & RR Spey   & 10/15/97\\
\bottomrule
\end{tabularx}

\begin{tablenotes}
\item Source is from this website: \url{https://www.sedl.org/afterschool/toolkits/science/pdf/ast_sci_data_tables_sample.pdf}
\end{tablenotes}
\end{sidewaystable}


\section{Potential implications}

Authors should provide some additional comments about potential, more broad-ranging implications of their work that are not directly related to the current focus of their manuscript. This section is meant to promote discussion on possible ways the findings or data presented might be used in or have a relationship with other areas of research that may not be directly apparent in the work. It is not meant to provide `proof of importance' of the work. Only to engender expansion of use to other areas.

Explicit personal opinions by the authors are permitted, but they should be made clear as such. References or related information to support the propositions should be included. These section should focus on work that can be done within the foreseeable future and specifically using the information within the manuscript, not provide speculation on how it will relate to far-reaching goals of the research area.

\section{Methods}

The methods section should include the design of the study, the type of materials involved, a clear description of all comparisons, and the type of analysis used, to enable replication of the work. Ease of reproducibility is one of the key criteria on which reviewers will be asked to comment, so we strongly advocate the use of the reporting checklists recommended by the \href{https://biosharing.org/}{BioSharing} network and workflow management systems such as \href{https:/galaxyproject.org/}{Galaxy} and \href{http://www.myexperiment.org/home}{MyExperiment} and container systems such as \href{https://www.docker.com/}{Docker}, to save the details of the methods to encourage reproducibility as well as conciseness, is also strongly encouraged. Our \href{http://gigadb.org/}{\textit{Giga}DB repository}{} and \href{http://galaxy.cbiit.cuhk.edu.hk/}{GigaGalaxy} server can also be used to archive data, workflows and snapshots of the code with an accompanying DOI. Our \href{https://github.com/gigascience/papers}{GitHub page} can also be used to host a dynamic forkable version of the code if the authors have not used a code repository themselves.

\emph{GigaScience} encourages and assists with the submission of detailed protocols to the open access repository \href{https://www.protocols.io/}{protocols.io}. Please enter the details into protocols.io, issue a DOI, and cite the protocols.io record from the Methods section.

Authors benefit greatly by posting their methods in protocols.io as these are in a formatted form, allow inclusion of all the details, are fully searchable unlike supplementary files, and can be updated to new versions as basic methodology changes over time. Doing this saves authors extensive time in the future as the methods do not need to be rewritten in future manuscripts as they need only be cited.

\section{Availability of source code and requirements (optional, if code is present)}

Lists the following:
\begin{itemize}
\item Project name: e.g.~My bioinformatics project
\item Project home page: e.g.~\url{http://sourceforge.net/projects/mged}
\item Operating system(s): e.g.~Platform independent
\item Programming language: e.g.~Java
\item Other requirements: e.g.~Java 1.3.1 or higher, Tomcat 4.0 or higher
\item License: e.g.~GNU GPL, FreeBSD etc.
Any restrictions to use by non-academics: e.g. licence needed
\end{itemize}

This needs to be under an \href{http:/opensource.org/licenses}{Open Source Initiative} approved license where practicable compiled running software is made available. If the code is not hosted in a repository the \href{https://github.com/gigascience}{\textit{GigaScience} GitHub repository} is also available for this purpose.

\section{Availability of supporting data and materials}

\textit{GigaScience} requires authors to deposit the data set(s) supporting the results reported in submitted manuscripts in a publicly-accessible data repository such as \href{http://gigadb.org/}{\textit{Giga}DB} (see \textit{Giga}DB database terms of use for complete details). This section should be included when supporting data are available and must include the name of the repository and the permanent identifier or accession number and persistent hyperlinks for the data sets (if appropriate). The following format is recommended:

``The data set(s) supporting the results of this article is(are) available in the [repository name] repository, [cite unique persistent identifier].''

Following the \href{https://www.force11.org/group/joint-declaration-data-citation-principles-final}{Joint Declaration of Data Citation Principles}, where appropriate we ask that the data sets be cited where it is first mentioned in the manuscript, and included in the reference list. If a DOI has been issued to a dataset please always cite it using the DOI rather than the less stable URL the DOI resolves to (e.g.~\url{http://dx.doi.org/10.5524/100044} rather than \url{http://gigadb.org/dataset/100044}). For more see:

Data Citation Synthesis Group: Joint Declaration of Data Citation Principles. Martone M. (ed.) San Diego CA: FORCE11; 2014 [\url{https://www.force11.org/datacitation}]

A list of available scientific research data repositories can be found in \href{http://www.re3data.org/}{res3data} and \href{https://biosharing.org/}{BioSharing}.

\section{Declarations}

\subsection{List of abbreviations}
If abbreviations are used in the text they should be defined in the text at first use, and a list of abbreviations should be provided in alphabetical order.

\subsection{Ethical Approval (optional)}
Manuscripts reporting studies involving human participants, human data or human tissue must:

\begin{itemize}
\item include a statement on ethics approval and consent (even where the need for approval was waived)
\item include the name of the ethics committee that approved the study and the committee's reference number if appropriate
\end{itemize}

Studies involving animals must include a statement on ethics approval and have been treated in a humane manner in line with the \href{http://www.nc3rs.org.uk/arrive-guidelines}{ARRIVE guidelines}.

See our \href{https://academic.oup.com/gigascience/pages/editorial_policies_and_reporting_standards}{editorial policies} for more information.

If your manuscript does not report on or involve the use of any animal or human data or tissue, this section is not applicable to your submission. Please state ``Not applicable'' in this section.

\subsection{Consent for publication}

If your manuscript contains any individual person's data in any form, consent to publish must be obtained from that person, or in the case of children, their parent or legal guardian. All presentations of case reports must have consent to publish. You can use your institutional consent form. You should not send the form to us on submission, but we may request to see a copy at any stage (including after publication). Please also confirm you have followed national guidelines on data collection and release in the place the research was carried out, for example confirming you have Ministry of Science and Technology (MOST) approval in China.

If your manuscript does not contain any individual person's data, please state ``Not applicable'' in this section.

\subsection{Competing Interests}

All financial and non-financial competing interests must be declared in this section. See our \href{https://academic.oup.com/gigascience/pages/editorial_policies_and_reporting_standards}{editorial policies} for a full explanation of competing interests. Where an author gives no competing interests, the listing will read `The author(s) declare that they have no competing interests'. If you are unsure whether you or any of your co-authors have a competing interest please contact the editorial office.


\subsection{Funding}

All sources of funding for the research reported should be declared. The role of the funding body in the design of the study and collection, analysis, and interpretation of data and in writing the manuscript should be declared. Please use \href{http://www.crossref.org/fundingdata/}{FundRef} to report funding sources and include the award/grant number, and the name of the Principal Investigator of the grant.


\subsection{Author's Contributions}

The individual contributions of authors to the manuscript should be specified in this section. Guidance and criteria for authorship can be found in our \href{https://academic.oup.com/gigascience/pages/editorial_policies_and_reporting_standards}{editorial policies}. We would recommend you follow some kind of standardised taxonomy like the \href{http://docs.casrai.org/CRediT}{CASRAI CRediT} (Contributor Roles Taxonomy).


\section{Acknowledgements}

Please acknowledge anyone who contributed towards the article who does not meet the criteria for authorship including anyone who provided professional writing services or materials.

Authors should obtain permission to acknowledge from all those mentioned in the Acknowledgements section. If you do not have anyone to acknowledge, please write ``Not applicable'' in this section.

See our \href{https://academic.oup.com/gigascience/pages/editorial_policies_and_reporting_standards}{editorial policies} for a full explanation of acknowledgements and authorship criteria.

Group authorship: if you would like the names of the individual members of a collaboration group to be searchable through their individual PubMed records, please ensure that the title of the collaboration group is included on the title page and in the submission system and also include collaborating author names as the last paragraph of the “Acknowledgements” section. Please add authors in the format First Name, Middle initial(s) (optional), Last Name. You can add institution or country information for each author if you wish, but this should be consistent across all authors.

Please note that individual names may not be present in the PubMed record at the time a published article is initially included in PubMed as it takes PubMed additional time to code this information.

\section{Authors' information (optional)}

You may choose to use this section to include any relevant information about the author(s) that may aid the reader's interpretation of the article, and understand the standpoint of the author(s). This may include details about the authors' qualifications, current positions they hold at institutions or societies, or any other relevant background information. Please refer to authors using their initials. Note this section should not be used to describe any competing interests.



%% Specify your .bib file name here, without the extension
\bibliography{paper-refs}

\begin{landscape}
\begin{table}
\caption{Automobile land speed records (GR 5-10). This is again the same table as before, but on a landscaped page. \textbf{Note that a hard page break is inserted immediately before and after \texttt{landscape}}, so you'll need to carefully position such an environment at a suitable location in your manuscript!}
\label{tab:example:landscape}
\begin{tabularx}{\linewidth}{S l l l r L}
\toprule
{Speed (mph)} & {Driver} & {Car} & {Engine} & {Date} & {Extra comments}\\
\midrule
407.447     & Craig Breedlove & Spirit of America          & GE J47    & 8/5/63  & (Just to demo a full-width table with auto-wrapping long lines) \\
413.199     & Tom Green       & Wingfoot Express           & WE J46    & 10/2/64  \\
434.22      & Art Arfons      & Green Monster              & GE J79    & 10/5/64  \\
468.719     & Craig Breedlove & Spirit of America          & GE J79    & 10/13/64 \\
526.277     & Craig Breedlove & Spirit of America          & GE J79    & 10/15/65 \\
536.712     & Art Arfons      & Green Monster              & GE J79    & 10/27/65 \\
555.127     & Craig Breedlove & Spirit of America, Sonic 1 & GE J79    & 11/2/65  \\
576.553     & Art Arfons      & Green Monster              & GE J79    & 11/7/65  \\
600.601     & Craig Breedlove & Spirit of America, Sonic 1 & GE J79    & 11/15/65 \\
622.407     & Gary Gabelich   & Blue Flame                 & Rocket    & 10/23/70 \\
633.468     & Richard Noble   & Thrust 2                   & RR RG 146 & 10/4/83  \\
763.035     & Andy Green      & Thrust SSC                 & RR Spey   & 10/15/97\\
\bottomrule
\end{tabularx}

\begin{tablenotes}
\item Source is from this website: \url{https://www.sedl.org/afterschool/toolkits/science/pdf/ast_sci_data_tables_sample.pdf}
\end{tablenotes}
\end{table}
\end{landscape}



\end{document}
